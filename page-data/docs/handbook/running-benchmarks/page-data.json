{
    "componentChunkName": "component---src-templates-documentation-tsx",
    "path": "/docs/handbook/running-benchmarks",
    "result": {"data":{"markdownRemark":{"id":"28793760-10e1-5032-868c-47fcb2f26747","excerpt":"Running Benchmarks The LF repository contains a series of benchmarks in the  directory. There is also a flexible benchmark runner that automates the process of…","html":"<h1 id=\"running-benchmarks\" style=\"position:relative;\"><a href=\"#running-benchmarks\" aria-label=\"running benchmarks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Running Benchmarks</h1>\n<p>The LF repository contains a series of benchmarks in the <code class=\"language-text\">benchmark</code> directory. There is also a flexible benchmark runner that automates the process of running benchmarks for various settings and collecting results from those benchmarks. It is located in <code class=\"language-text\">benchmark/runner</code>.\nThe runner is written in python and is based on <a href=\"https://hydra.cc/docs/intro\">hydra</a>, a tool for dynamically creating hierarchical configurations by composition</p>\n<h2 id=\"prerequisites\" style=\"position:relative;\"><a href=\"#prerequisites\" aria-label=\"prerequisites permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Prerequisites</h2>\n<h3 id=\"install-python-dependencies\" style=\"position:relative;\"><a href=\"#install-python-dependencies\" aria-label=\"install python dependencies permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Install Python dependencies</h3>\n<p>The benchmark runner is written in Python and requires a working Python3 installation. It also requires a few python packages to be installed. Namely, <code class=\"language-text\">hydra-core</code>, <code class=\"language-text\">cogapp</code> and <code class=\"language-text\">pandas</code>.</p>\n<p>It is recommended to install the dependencies and execute the benchmark runner in a virtual environment. For instance, this can be done with <code class=\"language-text\">virtualenv</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">virtualenv ~/virtualenvs/lfrunner -p python3\nsource ~/virtualenvs/lfrunner/bin/activate</code></pre></div>\n<p>Then the dependencies can be installed by running:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">pip install -r benchmark/runner/requirements.txt</code></pre></div>\n<h3 id=\"compile-lfc\" style=\"position:relative;\"><a href=\"#compile-lfc\" aria-label=\"compile lfc permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Compile lfc</h3>\n<p>For running LF benchmarks, the commandline compiler <code class=\"language-text\">lfc</code> needs to be built. Simply run</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">bin/build-lfc</code></pre></div>\n<p>in the root directory of the LF repository.</p>\n<p>Also, the environment variable <code class=\"language-text\">LF_PATH</code> needs to be set and point to the location of the LF repository. This needs to be an absolute path.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">export LF_PATH=/path/to/lf</code></pre></div>\n<h3 id=\"setup-savina\" style=\"position:relative;\"><a href=\"#setup-savina\" aria-label=\"setup savina permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Setup Savina</h3>\n<p>Currently all of our benchmarks are ported from the <a href=\"https://doi.org/10.1145/2687357.2687368\">Savina actor benchmark suite</a>. In order to compare our LF implementations with actor based implementation, the Savina benchmark suite needs to be downloaded and compiled. Note that we require a modified version of the Savina suite, that adds support for specifying the number of worker threads and that includes CAF implementations of most benchmarks.</p>\n<p>To download and build savina, run the following commands:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">git clone https://github.com/lf-lang/savina.git\ncd savina\nmvn install</code></pre></div>\n<p>Building Savina requires a Java 8 JDK. Depending on the local setup, <code class=\"language-text\">JAVA_HOME</code> might need to be adjusted before running <code class=\"language-text\">mvn</code> in order to point to the correct JDK.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">export JAVA_HOME=/path/to/jdk8</code></pre></div>\n<p>Before invoking the benchmark runner, the environment variable <code class=\"language-text\">SAVINA_PATH</code> needs to be set and point to the location of the savina repository using an absolute path.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">export SAVINA_PATH=/path/to/savina</code></pre></div>\n<h4 id=\"caf\" style=\"position:relative;\"><a href=\"#caf\" aria-label=\"caf permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>CAF</h4>\n<p>To futher build the CAF benchmarks, CAF 0.16.5 needs to be downloaded, compiled and installed first:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">git clone --branch \"0.16.5\" git@github.com:actor-framework/actor-framework.git\nmkdir actor-framework/build &amp;&amp; cd actor-framework/build\ncmake -DCMAKE_INSTALL_PREFIX=&lt;preferred/install/location> ..\nmake install</code></pre></div>\n<p>Then, from within the savina directory, the CAF benchmarks can be build:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">cmake -DCAF_ROOT_DIR=&lt;path/to/caf/install/location> ..\nmake</code></pre></div>\n<p>The CAF benchmarks are used in these two publications:</p>\n<ul>\n<li><a href=\"https://www.researchgate.net/publication/322519252_Reducing_Message_Latency_and_CPU_Utilization_in_the_CAF_Actor_Framework\">“Reducing Message Latency and CPU Utilization in the CAF Actor Framework”</a></li>\n<li><a href=\"https://link.springer.com/article/10.1007/s10766-020-00663-1%5D\">“Improving the Performance of Actors on Multi-cores with Parallel Patterns”</a></li>\n</ul>\n<h2 id=\"running-a-benchmark\" style=\"position:relative;\"><a href=\"#running-a-benchmark\" aria-label=\"running a benchmark permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Running a benchmark</h2>\n<p>A benchmark can simply be run by specifying a benchmark and a target. For instance</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">cd benchmark/runner\n./run_benchmark.py benchmark=savina_micro_pingpong target=lf-c</code></pre></div>\n<p>runs the Ping Pong benchmark from the Savina suite using the C-target of LF. Currently, supported targets are <code class=\"language-text\">lf-c</code>, <code class=\"language-text\">lf-cpp</code>, <code class=\"language-text\">akka</code>, and <code class=\"language-text\">caf</code> where <code class=\"language-text\">akka</code> corresponds to the Akka implementation in the original Savina suite and <code class=\"language-text\">caf</code> corresponds to a implementation using the <a href=\"https://www.actor-framework.org/\">C++ Actor Framework</a> .</p>\n<p>The benchmarks can also be configured. The <code class=\"language-text\">threads</code> and <code class=\"language-text\">iterations</code> parameters apply to every benchmark and specify the number of worker threads as well as how many times the benchmark should be run. Most benchmarks allow additional parameters. For instance, the Ping Pong benchmark sends a configurable number of pings that be set via the <code class=\"language-text\">benchmark.params.messages</code> configuration key. Running the Akka version of the Ping Pong benchmark for 1000 messages, 1 thread and 12 iterations could be done like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">./run_benchmark.py benchmark=savina_micro_pingpong target=akka threads=1 iterations=12 benchmark.params.messages=1000</code></pre></div>\n<p>Each benchmark run produces an output directory in the scheme <code class=\"language-text\">outputs/&lt;date>/&lt;time>/</code> (e.g. <code class=\"language-text\">outputs/2020-12-17/16-46-16/</code>). This directory contains a files <code class=\"language-text\">results.csv</code> which contains the measured execution time for each iteration and all the parameters used for running this particular benchmark. The csv file contains precisely one row per iteration.</p>\n<h2 id=\"running-a-series-of-benchmarks-multirun\" style=\"position:relative;\"><a href=\"#running-a-series-of-benchmarks-multirun\" aria-label=\"running a series of benchmarks multirun permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Running a series of benchmarks (multirun)</h2>\n<p>The runner also allows to automatically run a single benchmark or a series of benchmarks with a range of settings. The multirun feature is simply used by the <code class=\"language-text\">-m</code> switch. For instance:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">./run_benchmark.py -m benchmark=savina_micro_pingpong target=\"glob(*)\" threads=1,2,4 iterations=12 benchmark.params.messages=\"range(1000000,10000000,1000000)\"</code></pre></div>\n<p>runs the Ping Pong benchmark for all targets using 1, 2 and 4 threads and for a number of messages ranging from 1M to 10M (in 1M steps).</p>\n<p>This mechanism can also be used to run multiple benchmarks. For instance,</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">./run_benchmark.py -m benchmark=\"glob(*)\" target=\"glob(*)\" threads=4 iterations=12</code></pre></div>\n<p>runs all benchmarks for all targets using 4 threads and 12 iterations.</p>\n<p>The results for a multirun are written to a directory in the scheme <code class=\"language-text\">multirun/&lt;date>/&lt;time>/&lt;n></code> (e.g. <code class=\"language-text\">multirun/2020-12-17/17-11-03/0/</code>) where <code class=\"language-text\">&lt;n></code> denotes the particular run. Each of the <code class=\"language-text\">&lt;n></code> subdirectories contains a <code class=\"language-text\">results.csv</code> for this particular run.</p>\n<h2 id=\"collecting-results-from-multirun\" style=\"position:relative;\"><a href=\"#collecting-results-from-multirun\" aria-label=\"collecting results from multirun permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Collecting results from multirun</h2>\n<p>A second script called <code class=\"language-text\">collect_results.py</code> provides a convenient way for collecting results from a multirun and merging them into a single CSV file. Simply running</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">./collect_results.py multirun/&lt;date>/&lt;time>/ out.csv</code></pre></div>\n<p>collects all results from the particular multirun and stores the merged data structure in out.csv. <code class=\"language-text\">collect_results.py</code> not only merges the results, but it also calculates minimum, maximum and median execution time for each individual run. The resulting CSV does not contain the measured values of individual iterations anymore and only contains a single row per run. This behavior can be disabled with the <code class=\"language-text\">--raw</code> command line flag. With the flag set, the results from all runs are merged as say are and the resulting file contains rows for all individual runs, but no minimum, maximum and median values.</p>\n<h2 id=\"how-it-works\" style=\"position:relative;\"><a href=\"#how-it-works\" aria-label=\"how it works permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How it works</h2>\n<p>The benchmark runner itself is actually relatively simple. Most of the complexity is dealt with by <a href=\"https://hydra.cc/\">hydra</a>. Hydra is a complex and convienient tool for handling configurations. These configurations can be merged from different sources and be overriden via command line arguments as you have seen above. The actual benchmark runner receives the configuration represented as nested dictionaries from hydra. It then executes the benchmarks precisely as instructed by the configutation.</p>\n<p>The configuration is split into two big parts: the benchmark configuration and the target configuration. The benchmark configuration describes a particular benchmark instance. This is described in more detail in the <a href=\"#adding-new-benchmarks\">next section</a>. The target configuration specifies how to run a benchmark for a specific target (e.g. akka, lf-c, lf-cpp). This is not intended to be changed by the user and therefore isn’t explained in detail here. Essentially a benchmark run is split into 5 steps as is outlined in the following. The target configuartion precisely specifies what needs to be done in each step</p>\n<ol>\n<li><strong>copy</strong> The command used to copy relevant source files to a temporary directory.</li>\n<li><strong>gen</strong> The command used to generate a configured LF file. This is intended to apply a code generation tool like cog to the source code in order to make benchmarks parameterized.</li>\n<li><strong>compile</strong> The command used to compile the benchmark.</li>\n<li><strong>run</strong> The command used to generate the benchmark.</li>\n<li><strong>parser</strong> A parser (a python method) that is used to process the output of the benchmark run and that returns the execution times of individual benchmark runs in a list.</li>\n</ol>\n<h2 id=\"adding-new-benchmarks\" style=\"position:relative;\"><a href=\"#adding-new-benchmarks\" aria-label=\"adding new benchmarks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adding new benchmarks</h2>\n<p>In order to add new benchmarks, a new configuration file needs to be created in the <code class=\"language-text\">conf/benchmark</code> subdirectory. Benchmarks may be grouped by the underscore-delimited segments in their file name. For instance, the PingPong benchmark is part of the micro-benchmarks of the Savina suite, and consequently its configuration file is named in <code class=\"language-text\">conf/benchmark/savina_micro_pingpong.yaml</code>. This allows to later specify <code class=\"language-text\">benchmark=savina/micro/pingpong</code> on the command line. Below you can see the contents of <code class=\"language-text\">savina_micro_pingpong.yaml</code> which we will break down in the following.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token comment\"># @package benchmark</span>\n<span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Ping Pong\"</span>\n<span class=\"token key atrule\">params</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1000000</span>\n\n<span class=\"token comment\"># target specific configuration</span>\n<span class=\"token key atrule\">targets</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">akka</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">jar</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"${savina_path}/target/savina-0.0.1-SNAPSHOT-jar-with-dependencies.jar\"</span>\n    <span class=\"token key atrule\">class</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"edu.rice.habanero.benchmarks.pingpong.PingPongAkkaActorBenchmark\"</span>\n    <span class=\"token key atrule\">run_args</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"-n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"&lt;value>\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">caf</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">bin</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"caf_01_pingpong\"</span>\n    <span class=\"token key atrule\">run_args</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"-n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"&lt;value>\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">lf-cpp</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">copy_sources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token string\">\"${lf_path}/benchmark/Cpp/Savina/src/BenchmarkRunner.lf\"</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token string\">\"${lf_path}/benchmark/Cpp/Savina/src/micro\"</span>\n    <span class=\"token key atrule\">lf_file</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"micro/PingPong.lf\"</span>\n    <span class=\"token key atrule\">binary</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"PingPong\"</span>\n    <span class=\"token key atrule\">gen_args</span><span class=\"token punctuation\">:</span> <span class=\"token null important\">null</span>\n    <span class=\"token key atrule\">run_args</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"--count\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"&lt;value>\"</span><span class=\"token punctuation\">]</span>\n  <span class=\"token key atrule\">lf-c</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">copy_sources</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token string\">\"${lf_path}/benchmark/C/Savina/src/micro/PingPong.lf\"</span>\n    <span class=\"token key atrule\">lf_file</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"PingPong.lf\"</span>\n    <span class=\"token key atrule\">binary</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"PingPong\"</span>\n    <span class=\"token key atrule\">gen_args</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"-D\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"count=&lt;value>\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>The first line <code class=\"language-text\"># @package benchmark</code> is hydra specific. It specifies that this configuration is part of the benchmark package. Essentially this enables the configuration to be assigned to <code class=\"language-text\">benchmark</code> on the command line.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Ping Pong\"</span>\n<span class=\"token key atrule\">params</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token number\">1000000</span></code></pre></div>\n<p>This part sets the benchmark name to “Ping Pong” and declares that there is one benchmark specific parameter: <code class=\"language-text\">pings</code>. This configuration also set the default value for <code class=\"language-text\">pings</code> to 1000000. Note that the <code class=\"language-text\">params</code> dictionary may specify an arbitrary number of parameters.</p>\n<p>The remainder of the configuration file contains target specific configurations that provide instructions on how the particular benchmark can be run for the various targets. This block</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token comment\"># target specific configuration</span>\n<span class=\"token key atrule\">targets</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">akka</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">jar</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"${savina_path}/target/savina-0.0.1-SNAPSHOT-jar-with-dependencies.jar\"</span>\n    <span class=\"token key atrule\">class</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"edu.rice.habanero.benchmarks.pingpong.PingPongAkkaActorBenchmark\"</span>\n    <span class=\"token key atrule\">run_args</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"-n\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"&lt;value>\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>specifies how the benchmark is executed using Akka. The <code class=\"language-text\">jar</code> and <code class=\"language-text\">class</code> configuration keys simply instruct the benchmark runner which class in which jar to run. Note that hydra automatically resolves <code class=\"language-text\">${savina_path}</code> to the value you set in the <code class=\"language-text\">SAVINA_PATH</code> environment variable.</p>\n<p>The <code class=\"language-text\">run_args</code> configuration key allows specification of further arguments that are added to the command to be executed when running the benchmark. It expects a dictionary, where the keys are names of parameters as specified above in the <code class=\"language-text\">params</code> configuration key, and the values are a list of arguments to be added to the executed command. In the case of the <code class=\"language-text\">pings</code> parameter, the Akka implementation of the benchmark expects the <code class=\"language-text\">-n</code> flag followed by the parameter value. Note that the special string <code class=\"language-text\">&lt;value></code> is automatically resolved by the runner to the actual parameter value when executing the command.</p>\n<p>Instructions for the C++ target are specified as follows.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">lf-cpp</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">copy_sources</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token string\">\"${lf_path}/benchmark/Cpp/Savina/src/BenchmarkRunner.lf\"</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token string\">\"${lf_path}/benchmark/Cpp/Savina/src/micro\"</span>\n  <span class=\"token key atrule\">lf_file</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"micro/PingPong.lf\"</span>\n  <span class=\"token key atrule\">binary</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"PingPong\"</span>\n  <span class=\"token key atrule\">gen_args</span><span class=\"token punctuation\">:</span> <span class=\"token null important\">null</span>\n  <span class=\"token key atrule\">run_args</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"--count\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"&lt;value>\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>For C and C++ programs, we cannot run a precompiled program as it is the case for Akka, but we need to compile the benchmark first. The benchmark handler automatically performs the build in a temporary directory, so that it doesn’t interfere with the source tree. First, it copies all files listed under <code class=\"language-text\">copy_sources</code> to the temporary directory. If the specified source path is a directory, the whole directory is copied recursively. The <code class=\"language-text\">lf_file</code> configuration file specifies the file to be compiled with <code class=\"language-text\">lfc</code>. <code class=\"language-text\">binary</code> indicates the name of the binary file resulting from the compilation process.</p>\n<p>For some benchmarks, not all parameters can be applied at runtime. In such cases, the <code class=\"language-text\">gen_args</code> configuration key can be used to provide additional arguments that should be passed to cog. cog then applies the parameters to the source file (assuming that the source LF file uses cog directives to generate code according to the configuration). Similiarly <code class=\"language-text\">run_args</code> specifies any additional arguments that should be passed to the binary when running the benchmark. In the case of the C++ configuration for the Ping Pong benchmark, the number of pings is a runtime parameter and specified with <code class=\"language-text\">--count</code>. Since this particular benchmark does not have any paremeter that need to be set during generation, <code class=\"language-text\">gen_args</code> is set to <code class=\"language-text\">null</code>.</p>\n<p>Finally, we have the C part of the target configuration.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">lf-c</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">copy_sources</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">-</span> <span class=\"token string\">\"${lf_path}/benchmark/C/Savina/src/micro/PingPong.lf\"</span>\n  <span class=\"token key atrule\">lf_file</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"PingPong.lf\"</span>\n  <span class=\"token key atrule\">binary</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"PingPong\"</span>\n  <span class=\"token key atrule\">gen_args</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">pings</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"-D\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"count=&lt;value>\"</span><span class=\"token punctuation\">]</span></code></pre></div>\n<p>This is very similar to the C++ configuration. However, the C target of LF currently does not support overriding of parameter values at runtime. Therefore, all parameters need to be provided as arguments to the code generator and the benchmark needs to provide corresponding cog directives.</p>\n<p>New benchmarks can be simply added by replicating this example and adjusting the precise configuration values and parameters to the specific benchmark.</p>","headings":[{"value":"Running Benchmarks","depth":1},{"value":"Prerequisites","depth":2},{"value":"Install Python dependencies","depth":3},{"value":"Compile lfc","depth":3},{"value":"Setup Savina","depth":3},{"value":"CAF","depth":4},{"value":"Running a benchmark","depth":2},{"value":"Running a series of benchmarks (multirun)","depth":2},{"value":"Collecting results from multirun","depth":2},{"value":"How it works","depth":2},{"value":"Adding new benchmarks","depth":2}],"frontmatter":{"permalink":"/docs/handbook/running-benchmarks","title":"Running Benchmarks","oneline":"Running Benchmarks.","preamble":""}},"prev":{"childMarkdownRemark":{"frontmatter":{"title":"Developer IntelliJ Setup (for Kotlin)","oneline":"Developer IntelliJ Setup (for Kotlin).","permalink":"/docs/handbook/intellij-kotlin"}}},"next":{"childMarkdownRemark":{"frontmatter":{"title":"Contributing","oneline":"Contribute to Lingua Franca.","permalink":"/docs/handbook/contributing"}}}},"pageContext":{"id":"4-running-benchmarks","slug":"/docs/handbook/running-benchmarks","repoPath":"/packages/documentation/copy/en/developer/Running Benchmarks.md","previousID":"2c95dd57-3193-579b-9968-1a8e44b90737","lang":"en","modifiedTime":"2022-04-28T16:58:08.903Z"}},
    "staticQueryHashes": []}